import { useState, useEffect, useRef } from 'react'
import * as ort from 'onnxruntime-web'
import type { AnimalConfig } from '../types/animal-config'

/**
 * Network metadata loaded from network_metadata.json.
 * Generated by convert_checkpoint.py during model conversion.
 */
export interface NetworkMetadata {
  observation_size: number
  reference_obs_size: number
  proprioceptive_obs_size: number
  action_size: number
  latent_size: number
  encoder_layer_sizes: number[]
  decoder_layer_sizes: number[]
}

interface UseONNXResult {
  session: ort.InferenceSession | null
  decoderSession: ort.InferenceSession | null
  highlevelSession: ort.InferenceSession | null
  metadata: NetworkMetadata | null
  isReady: boolean
  error: string | null
}

/**
 * Derive metadata path from ONNX path (same directory).
 */
function getMetadataPath(onnxPath: string): string {
  // Remove query params and get directory
  const pathWithoutQuery = onnxPath.split('?')[0]
  const dir = pathWithoutQuery.substring(0, pathWithoutQuery.lastIndexOf('/'))
  return `${dir}/network_metadata.json`
}

/**
 * Validate loaded metadata against config expectations.
 */
function validateMetadata(metadata: NetworkMetadata, config: AnimalConfig): void {
  const warnings: string[] = []

  if (metadata.observation_size !== config.obs.totalSize) {
    warnings.push(`observation_size mismatch: metadata=${metadata.observation_size}, config=${config.obs.totalSize}`)
  }
  if (metadata.reference_obs_size !== config.obs.referenceObsSize) {
    warnings.push(`reference_obs_size mismatch: metadata=${metadata.reference_obs_size}, config=${config.obs.referenceObsSize}`)
  }
  if (metadata.proprioceptive_obs_size !== config.obs.proprioceptiveObsSize) {
    warnings.push(`proprioceptive_obs_size mismatch: metadata=${metadata.proprioceptive_obs_size}, config=${config.obs.proprioceptiveObsSize}`)
  }
  if (metadata.action_size !== config.action.size) {
    warnings.push(`action_size mismatch: metadata=${metadata.action_size}, config=${config.action.size}`)
  }

  if (warnings.length > 0) {
    console.warn('Network metadata validation warnings:')
    warnings.forEach(w => console.warn(`  - ${w}`))
  }
}

export function useONNX(config: AnimalConfig): UseONNXResult {
  const [session, setSession] = useState<ort.InferenceSession | null>(null)
  const [decoderSession, setDecoderSession] = useState<ort.InferenceSession | null>(null)
  const [highlevelSession, setHighlevelSession] = useState<ort.InferenceSession | null>(null)
  const [metadata, setMetadata] = useState<NetworkMetadata | null>(null)
  const [isReady, setIsReady] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const initRef = useRef(false)
  const configIdRef = useRef(config.id)

  useEffect(() => {
    // Reset if config changed
    if (configIdRef.current !== config.id) {
      configIdRef.current = config.id
      initRef.current = false
      setIsReady(false)
      setError(null)
      setMetadata(null)
      setHighlevelSession(null)
    }

    if (initRef.current) return
    initRef.current = true

    async function init() {
      try {
        // Configure ONNX Runtime for WASM backend
        ort.env.wasm.wasmPaths = 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.23.2/dist/'

        const sessionOptions = {
          executionProviders: ['wasm'] as const,
          graphOptimizationLevel: 'all' as const,
        }

        // Load metadata, full model, decoder model, and high-level model in parallel
        const metadataPath = getMetadataPath(config.assets.onnxPath)
        const [metadataResponse, onnxSession, decoderOnnxSession, highlevelOnnxSession] = await Promise.all([
          fetch(metadataPath).then(r => {
            if (!r.ok) throw new Error(`Failed to load network metadata: ${r.status}`)
            return r.json() as Promise<NetworkMetadata>
          }),
          ort.InferenceSession.create(config.assets.onnxPath, sessionOptions),
          config.assets.decoderOnnxPath
            ? ort.InferenceSession.create(config.assets.decoderOnnxPath, sessionOptions)
            : Promise.resolve(null),
          config.joystick?.highlevelOnnxPath
            ? ort.InferenceSession.create(config.joystick.highlevelOnnxPath, sessionOptions)
            : Promise.resolve(null),
        ])

        // Validate metadata against config
        validateMetadata(metadataResponse, config)

        setMetadata(metadataResponse)
        setSession(onnxSession)
        setDecoderSession(decoderOnnxSession)
        setHighlevelSession(highlevelOnnxSession)
        setIsReady(true)

        console.log(`ONNX Runtime initialized for ${config.name}`)
        console.log('  Network metadata:', metadataResponse)
        console.log('  Full model inputs:', onnxSession.inputNames)
        console.log('  Full model outputs:', onnxSession.outputNames)
        if (decoderOnnxSession) {
          console.log('  Decoder model inputs:', decoderOnnxSession.inputNames)
          console.log('  Decoder model outputs:', decoderOnnxSession.outputNames)
        }
        if (highlevelOnnxSession) {
          console.log('  High-level model inputs:', highlevelOnnxSession.inputNames)
          console.log('  High-level model outputs:', highlevelOnnxSession.outputNames)
        }
      } catch (e) {
        console.error('Failed to initialize ONNX Runtime:', e)
        setError(e instanceof Error ? e.message : 'Failed to initialize ONNX Runtime')
      }
    }

    init()
  }, [config])

  return { session, decoderSession, highlevelSession, metadata, isReady, error }
}

/**
 * Result from policy network inference.
 */
export interface InferenceResult {
  logits: Float32Array
  latent: Float32Array
}

/**
 * Run inference on the policy network.
 * Input: observation vector (size from config.obs.totalSize)
 * Output: logits (action means + action log_stds) and latent vector
 */
export async function runInference(
  session: ort.InferenceSession,
  observation: Float32Array
): Promise<InferenceResult> {
  const inputTensor = new ort.Tensor('float32', observation, [1, observation.length])
  const feeds = { obs: inputTensor }
  const results = await session.run(feeds)
  const logits = results['action_logits'].data as Float32Array
  const latent = results['latent_mean'].data as Float32Array
  return { logits, latent }
}

/**
 * Extract action from network output into pre-allocated buffer.
 * Takes the first actionSize dims (mean) and applies tanh.
 * Writes result to `out` buffer to avoid per-frame allocations.
 */
export function extractActionInto(out: Float32Array, logits: Float32Array): void {
  for (let i = 0; i < out.length; i++) {
    out[i] = Math.tanh(logits[i])
  }
}

/**
 * Run inference on the decoder-only network (for latent walk mode).
 * Input: latent vector + proprioceptive observation
 * Output: logits (action means + action log_stds)
 */
export async function runDecoderInference(
  session: ort.InferenceSession,
  latent: Float32Array,
  proprioObs: Float32Array
): Promise<Float32Array> {
  const latentTensor = new ort.Tensor('float32', latent, [1, latent.length])
  const proprioTensor = new ort.Tensor('float32', proprioObs, [1, proprioObs.length])
  const feeds = { latent: latentTensor, proprio_obs: proprioTensor }
  const results = await session.run(feeds)
  const output = results[session.outputNames[0]]
  return output.data as Float32Array
}

/**
 * Run inference on the high-level policy network (for joystick mode).
 * Input: task observation vector (prev_action + sensors + command)
 * Output: latent vector (already has tanh applied)
 */
export async function runHighlevelInference(
  session: ort.InferenceSession,
  taskObs: Float32Array
): Promise<Float32Array> {
  const tensor = new ort.Tensor('float32', taskObs, [1, taskObs.length])
  const feeds = { task_obs: tensor }
  const results = await session.run(feeds)
  return results['latent'].data as Float32Array
}
